1 слайд

Разработка полносвязной нейронной сети на базе фреймворка TensorFlow  Python для анализа индикаторов ментального здоровья студента

выполнили студенты группы 15.27Д-ПИ02/25б Максимов Даниил Андреевич, Смыгалин Артемий Сергеевич



2 слайд

Депрессия — это психическое расстройство, при котором у человека длительно (как минимум несколько недель) сохраняются подавленное настроение, утрата интереса к привычной деятельности и снижение энергии. Это не просто «плохое настроение», а состояние, которое нарушает повседневную жизнь, работу, учёбу и отношения с людьми. Часто депрессия сопровождается нарушением сна и аппетита, чувством вины и бесполезности, трудностями с концентрацией и возможными суицидальными мыслями, поэтому требует профессиональной помощи специалистов в области психического здоровья.

Цель данного проекта — с применением методов машинного обучения и интерпретации данных построить модель, способную прогнозировать вероятность развития депрессии у студентов


3 слайд 
В коде представлена полносвязная нейронная сеть (Fully Connected Neural Network) с последовательной архитектурой (Sequential). Общая структура:

· Тип: Feed-forward нейронная сеть (полносвязная) · Количество слоев: 5 (1 входной + 3 скрытых + 1 выходной) · Активация: ReLU для скрытых слоев, Sigmoid для выходного · Функция потерь: Binary Crossentropy (бинарная классификация) · Выход: риск развития депрессии (0-100%)


4 слайд

Архитектура модели

входной слой - скрытые слои(4)- выходной слой

Подробная схема слоев:

Входной слой:

Форма: (n_features,) где n_features = количество признаков после обработки данных
Автоматически создается TensorFlow

Скрытый слой 1:

Dense(256, activation='relu') - 256 нейронов, функция активации ReLU
BatchNormalization() - нормализация пакетов
Dropout(0.3) - регуляризация дропаутом с вероятностью 30%

Скрытый слой 2:

Dense(128, activation='relu') - 128 нейронов, ReLU
BatchNormalization()
Dropout(0.25) - дропаут 25%

Скрытый слой 3:

Dense(64, activation='relu') - 64 нейрона, ReLU
BatchNormalization()
Dropout(0.2) - дропаут 20%

Скрытый слой 4:

Dense(32, activation='relu') - 32 нейрона, ReLU
BatchNormalization()
Dropout(0.15) - дропаут 15%

Выходной слой:

Dense(1, activation='sigmoid') - 1 нейрон, сигмоида для бинарной классификации



5 слайд 

Ключевые характеристики архитектуры

1. Тип архитектуры:
**Последовательная ** - слои идут один за другим
Полносвязная (Fully Connected/Dense) - каждый нейрон связан со всеми нейронами следующего слоя
2. Паттерн уменьшения нейронов:
256 → 128 → 64 → 32 → 1
Типичный паттерн "воронки" (funnel pattern), где количество нейронов уменьшается в 2 раза на каждом слое.

3. Регуляризация:
Dropout: Вероятность уменьшается с глубиной (0.3 → 0.25 → 0.2 → 0.15)
Batch Normalization: После каждого Dense слоя (кроме выходного)
4. Функции активации:
Скрытые слои: ReLU (Rectified Linear Unit)
Выходной слой: Sigmoid (для бинарной классификации)
5. Оптимизатор:
Adam с кастомными параметрами (learning_rate=0.0005)
6. Функция потерь:
binary_crossentropy (бинарная кросс-энтропия)
7. Метрики:
Accuracy (точность)

Примерное количество параметров (для 10 входных признаков):

Слой 1: (10 × 256) + 256 = ~2,816

Слой 2: (256 × 128) + 128 = ~32,896

Слой 3: (128 × 64) + 64 = ~8,256

Выходной: (32 × 1) + 1 = 33

Итого: ~44,001 параметров

Это классическая архитектура для задач бинарной классификации со средним количеством признаков


слайд 6

Алгоритмы

Тип алгоритма: Алгоритм на основе обратного распространения (Backpropagation-based)

Как работает:


class GradientFlowController:
    def get_callbacks(self):
        # Возвращает три алгоритма контроля обучения

Три встроенных алгоритма:

Алгоритм ранней остановки (EarlyStopping):

Мониторит val_accuracy
Останавливает обучение если нет улучшений 15 эпох
Восстанавливает лучшие веса

Алгоритм уменьшения скорости обучения (ReduceLROnPlateau):

Мониторит val_loss
Уменьшает learning_rate в 2 раза если нет улучшений 8 эпох
Минимальный learning_rate = 0.00001

Алгоритм сохранения модели (ModelCheckpoint):

Сохраняет лучшую модель по val_accuracy
Автоматическая загрузка лучших весов



слайд 7-8 

описание гиперпараметров модели




| № слоя | Тип слоя         | Кол-во нейронов        | Функция активации | Дополнительно                              |
|-------|------------------|------------------------|-------------------|--------------------------------------------|
| 1     | Входной          | `X_train.shape[1]`     | —                 | Стандартизация (StandardScaler)           |
| 2     | Dense            | 256                    | ReLU              | BatchNormalization + Dropout 0.3          |
| 3     | Dense            | 128                    | ReLU              | BatchNormalization + Dropout 0.25         |
| 4     | Dense            | 64                     | ReLU              | BatchNormalization + Dropout 0.2          |
| 5     | Dense            | 32                     | ReLU              | BatchNormalization + Dropout 0.15         |
| 6     | Выходной Dense   | 1                      | Sigmoid           | Бинарная классификация (риск депрессии)   |

**Гиперпараметры**

- Функция потерь: Binary Crossentropy (бинарная кросс‑энтропия).  
- Оптимизатор: Adam, learning rate = 0.0005, beta1 = 0.9, beta2 = 0.999, epsilon = 1e‑7.  
- Метрика: accuracy.  
- Регуляризация: Dropout (0.3 / 0.25 / 0.2 / 0.15) + BatchNormalization в скрытых слоях.

